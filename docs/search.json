[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Capstone in Political Economy",
    "section": "",
    "text": "Introduction\nThis web book is meant to be a one-stop resource for all data and analysis related tasks in the Capstone in Political Economy course.\nThe pages will correspond to Data and Analysis sections in the course.\nThe notes here will use R as a programming language and RStudio as platform (or technically known as IDE - Integrated Development Platform) of choice. However, all the concepts will also be valid for Stata users. I will include any important/relevant deviations between the two programming statistical software approaches here.\nYou should start by having the following installed on your computers:\n\nR - Statistical Programming Language\nRStudio - Interactive Development Environment for R\nStata - Statistical Software Package (the â€œotherâ€ one)\nZotero - Reference Management Tool\nGitHub - Storing code online with version control\n\n\nR and R Studio\nUse the link here to install RStudio on your systems.\n\n\nStata\nStata is a paid software. Georgetown provides download option through the university webstore here.\n\n\nZotero\nZotero is a reference management tool. It allows you to maintain a structured bibliography. Its integration with various web browsers and software like MS Word and R, makes it a fantastic tool for keeping track of readings as well as citing them.\n\n\n\n\n\nZotero Folder\n\n\n\n\n\nMake Zotero Account.\nAdd a connector to the browser (For eg, zotero connector for google chrome).\nDownload Zotero Desktop App here.\nConnect Zotero to R Studio. (We will do this step on Day 5, using the link here).\n\n\n\nGitHub\nMake your account here",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Course Details",
    "section": "",
    "text": "Capstone in Political Economy\nIPEC/PECOâ€”4980, Spring 2025\nTuesday/Thursday: 11:00â€“12:15\nWalsh 495",
    "crumbs": [
      "Course Details"
    ]
  },
  {
    "objectID": "intro.html#professors",
    "href": "intro.html#professors",
    "title": "Course Details",
    "section": "Professors",
    "text": "Professors\nProf.Â Nita Rudra\n- Office: Mortara Center 206\n- Office Hours: Tuesdays, 12:30â€“1:30 pm\n- Email: nr404@georgetown.edu\nProf.Â Joel Simmons\n- Office: Mortara Center #212\n- Office Hours: TBA\n- Email: js4618@georgetown.edu\nTA: Parushya\n- Office Hours: Thursdays, 1:00â€“3:00 PM\n- Location: TBA/Zoom - Email: pp714@georgetown.edu",
    "crumbs": [
      "Course Details"
    ]
  },
  {
    "objectID": "intro.html#office-hour-links",
    "href": "intro.html#office-hour-links",
    "title": "Course Details",
    "section": "Office Hour Links",
    "text": "Office Hour Links\n\nSign up for Professor Rudraâ€™s office hours (Zoom or in-person)\n\nSign up for Professor Simmonsâ€™ office hours",
    "crumbs": [
      "Course Details"
    ]
  },
  {
    "objectID": "intro.html#goals",
    "href": "intro.html#goals",
    "title": "Course Details",
    "section": "Goals",
    "text": "Goals\nStudents will write a senior research thesis in political economy. You will find and evaluate scholarly literature and make an original contribution to that literature. The focus will be on developing a research question and a methodology to address it. The course serves as the culmination of the knowledge you obtained as majors in International Political Economy (SFS students) or Political Economy (College students).",
    "crumbs": [
      "Course Details"
    ]
  },
  {
    "objectID": "intro.html#requirements",
    "href": "intro.html#requirements",
    "title": "Course Details",
    "section": "Requirements",
    "text": "Requirements\nThe only course requirement is that students attend and participate in all lecture, discussion, presentation, and writing sessions.\nStudents may find the following book helpful as they proceed, and so it is recommended but not required:\n\nPowner, L. (2015). Empirical research and writing. A Political Science Studentâ€™s Practical Guide. SAGE Publications, Inc.\nhttps://doi.org/10.4135/9781483395906",
    "crumbs": [
      "Course Details"
    ]
  },
  {
    "objectID": "intro.html#assignments",
    "href": "intro.html#assignments",
    "title": "Course Details",
    "section": "Assignments",
    "text": "Assignments\n\nResearch Paper (90%)\nEach student will write an article-length paper based on the studentâ€™s own original research. The length should be of a standard article in the political economy fieldâ€”about 20 pages, double-spaced, 12-point font, including tables, figures, references, and notes. In the paper, students are expected to evaluate, critique, test, and build upon a current debate in political economy. Students should develop hypotheses and test them using quantitative methods. All final thesis papers must be posted on the Canvas discussion site and submitted in hard copy (if possible) by the due date listed in the outline. Late papers will be downgraded Â½ a letter grade per day late.\n\n\nResearch Presentations (10%)\nToward the end of the term, each student will present their paper to the class. Student presentations should be no more than 8â€“10 minutes in length and will be followed by 8â€“10 minutes of questions and answers. Use of PowerPoint slides or other presentation materials is required. An electronic version of your presentation must be posted on the Canvas discussion site by 10:00 am on the day it is to be presented.",
    "crumbs": [
      "Course Details"
    ]
  },
  {
    "objectID": "intro.html#class-structure",
    "href": "intro.html#class-structure",
    "title": "Course Details",
    "section": "Class Structure",
    "text": "Class Structure\nThe course has three forms of instruction:\n\nLecture/Discussions: Sessions indicated as â€˜Discussionsâ€™ will be a cross between a lecture and group Q&A. These sessions will focus on the major components of the final thesis: Topic/Puzzle, literature review, methodology, and analysis.\n\nData Sessions: The TA will provide sessions aimed at providing you a brief refresher on working with statistical software such as R or Stata.\n\nWriting Days: Much of the semester will be dedicated toward group meetings and writing days. Students will come to class and work on their papers. The professors and TA will be available for questions.\n\nStudents will be divided into two cohorts: one led by Professor Nita Rudra and the other by Professor Joel Simmons. Each cohort will further be subdivided into Group A and Group B. Please ensure you are aware of your assigned cohort and group, as this will determine your participation and schedule throughout the course.\n\n\n\nTentative Schedule\n\n\nOn days scheduled as â€œWriting Day with Professorsâ€, the particular groups (A or B) from Prof Rudra & Simmons sections will meet them separately. The other groups from both sections will meet the TA (Writing Day with TA).\nAs the course progresses, writing days could also be used as days for one-on-one meetings with Professors and TA for focused discussion on your research projects.\nThe due dates for deliverables (column 3 above) are for both groups in both the sections. All the deliverables are due on Canvas before the beginning of class on that particular date.\nDetails of these deliverables is as follows:\nShort research proposal: Your research proposal should include (i) the research question and why it is interesting, (ii) your working hypothesis and its basis, (iii) potential methodological approach, data and challenges.Â  It is due on Canvas by the beginning of class.Â \nLiterature Review: Your literature review should summarize key scholarly works relevant to your research question, identify gaps or unresolved questions in the existing literature, and explain how your research will address these gaps.\nAnalysis: Submit a write-up of the analysis section (including the description of the research methods, presentation of results, and discussion of their robustness).\nPresentation: Standard conference poster presentation. More details about this deliverable to be explained in class.\nAnnotated replication material: Your annotated replication material should include all data, program files, and a detailed readme file necessary to replicate your results. The readme file should clearly explain the steps to reproduce your analysis and any specific instructions for using the data or code.\nFinal Paper: Your final paper should be an article-length manuscript (approximately 20 double-spaced pages) that evaluates, critiques, tests, and builds upon a current debate in political economy. It should include a clear research question, hypotheses, methodology, analysis, results, and a discussion of findings.",
    "crumbs": [
      "Course Details"
    ]
  },
  {
    "objectID": "using-R.html",
    "href": "using-R.html",
    "title": "Using R",
    "section": "",
    "text": "Workflow and File Management\nHereâ€™s a quick workflow for starting a new project, which incorporates the key ideas of Reproducible Research:\nFigureÂ 1: To create new project: (top) first click New Directory, then (middle) click New Project, then (bottom) fill in the directory (project) name, choose a good subdirectory for its home and click Create Project. source\nThere are three building blocks in a .qmd file:",
    "crumbs": [
      "Reproducible Research",
      "Using R"
    ]
  },
  {
    "objectID": "using-R.html#workflow-and-file-management",
    "href": "using-R.html#workflow-and-file-management",
    "title": "Using R",
    "section": "",
    "text": "Open Rstudio and create a new Rstudio Project by clicking File &gt; New Project. Name it peco-4980-thesis.\n\n\n\nCheck if now your RStudio Window shows the project name on top right corner. If not, go to folder and double-click the .RProj file.\nDownload the 000-setup.R from here. Paste the 000-setup.R file in the main project folder. Open it in the same Rstudio window with the project and run the complete file. Your folder structure is created.\nCopy your raw data in Data/Raw folder. Similarly, your scripts if any, in Scripts/RScripts folder.\nStart your new .qmd file and save it in the main folder. Quarto is an in-built statistical programming and typesetting tool in R. The files are saved as .qmd files.\n\n\n\n\n\n\n\nA Quarto document is saved as a .qmd file. You can edit this file in two ways: Programmatically by being in source button and visually by choosing the Visual button, both button on top left corner of the .qmd window. More details about workign with Quarto can be found on the quarto website here.\n\n\n\n\n\n\nUse here() package extensively in both, scripts and quarto file, when loading or saving the data. The example below in variable creation and merging section illustrates its usage.\nYou can always zip the whole project folder for sharing. The receiver will just need to unzip and run the code after starting the associated .RProj file, without changing file paths on their computer.",
    "crumbs": [
      "Reproducible Research",
      "Using R"
    ]
  },
  {
    "objectID": "using-R.html#variable-creation-and-merging-datasets",
    "href": "using-R.html#variable-creation-and-merging-datasets",
    "title": "Using R",
    "section": "Variable Creation and Merging Datasets",
    "text": "Variable Creation and Merging Datasets\n\nClose the RStudio Window. Download the dataset for practice exercise from here. Unzip if needed and then paste the dataset inside ../Data/Raw/.\nClick on the .RProj file in the folder you just created.\nOpen a new .qmd file. Save it (ctrl/cmd + s) as practice.qmd in the root/main folder of the project.\nCope and paste the following code by creating a new R chunk in the qmd file. You can start a new R code chunk by pressing cmd + option + I or ctrl + alt + I.\n\n```{r}\n\ninstall.packages(\"here\")\nlibrary(here)\n\n\n # See the output for each of the following lines | Use your own datasets\nhere()\n\n# Make modification here after copying your dataset to this folder\n\nhere(\"Data\",\"Raw\",\"practice-datasets\",\"V-Dem-CY-Full+Others-v12.rds\")\n\n# syntax is\n\n# here(\"First subfolder from the root folder\", \"second subfolder\",...., \"file\")\n\n\nvdem_new &lt;- readRDS(here(\"Data\",\"Raw\",\"practice-datasets\",\"V-Dem-CY-Full+Others-v12.rds\"))\n\n```\n\nLoad the other datasets (in simultated-datasets folder) by modifying the syntax above and storing it in objects economic_data_2000_2020 and economic_data_1995_2024. Note how you have to change file paths as well as use different functions to load different file types (here dta and rds)\n\n```{r}\ninstall.packages(\"readstata13\") # Package for loading stata files in R\ninstall.packages(\"tidyverse\") # Package for data wrangling\n\nlibrary(tidyverse)\nlibrary(readstata13)\n\neconomic_data_2000_2020 &lt;- read_csv(here(\"Data/Raw/practice-datasets/simulated-datasets/Economic_Data_2000_2020.csv\"))\n\neconomic_data_1995_2024 &lt;- read.dta13(here(\"Data/Raw/practice-datasets/simulated-datasets/Economic_Data_1995_2024.dta\"))\n```\n\nInspect the datasets. There are multiple entries for each country corresponding to different years. Use the following code to merge and clean them.\n\n```{r}\n# Merge the datasets using a left join on 'country_code' and 'year'\nmerged_data &lt;- left_join(economic_data_1995_2024,economic_data_2000_2020, by = c(\"country_code\", \"year\"))\n\n# Filter out rows with NA values in the GDP columns which indicate non-joined rows\nfiltered_data &lt;- merged_data %&gt;%\n  filter(!is.na(gdp.x) & !is.na(gdp.y))\n\n# View the filtered data\nprint(filtered_data)\n\n```\n\nAfter merging create a new variable gdp_pc\n\n```{r}\n# Assuming 'gdp.x' is the GDP from the first dataset and 'population' is the population variable\nfiltered_data &lt;- filtered_data %&gt;%\n  mutate(gdp_pc = ifelse(!is.na(gdp.x) & !is.na(population) & population &gt; 0, gdp.x / population, NA))\n\n# View the updated data with the new 'gdp_pc' column\nprint(filtered_data)\n```\n\nSave the new dataset inside your project folder.\n\n\n```{r}\nwrite_csv(filtered_data, file = here(\"Data/Clean/country-year-gdppc.csv\"))\n```\n\nSave the file and close the project.",
    "crumbs": [
      "Reproducible Research",
      "Using R"
    ]
  },
  {
    "objectID": "using-stata.html",
    "href": "using-stata.html",
    "title": "Using STATA",
    "section": "",
    "text": "Workflow and File Management\nHereâ€™s a quick workflow for starting a new project, which incorporates the key ideas of reproducible research using Stata:",
    "crumbs": [
      "Reproducible Research",
      "Using STATA"
    ]
  },
  {
    "objectID": "using-stata.html#workflow-and-file-management",
    "href": "using-stata.html#workflow-and-file-management",
    "title": "Using STATA",
    "section": "",
    "text": "Create a new folder with name peco-4980-thesis.\nOpen your text editor or Stata and create a new do file. Name it 000-setup-mac.do to prepare the project directories. Or you can download the same file from here and paste it in the folder.\nSave and run the do file within Stata to set up your project directories. Code below is just for review from the same do file.\n\n```{stata}\n* Define the base project directory | According to your system\nglobal project_dir \"/path/to/your/project/directory\"\n\n* Function to create directories\ncap program drop create_folder\nprogram define create_folder\n    args folder_path\n    if !fileexists(\"`folder_path'\") {\n        mkdir \"`folder_path'\"\n    }\nend\n\n* Create Main Folders\ncreate_folder \"$project_dir/Data\"\ncreate_folder \"$project_dir/Data/Raw\"\ncreate_folder \"$project_dir/Data/Clean\"\n\ncreate_folder \"$project_dir/Scripts\"\ncreate_folder \"$project_dir/Scripts/Stata-Scripts\"\n\ncreate_folder \"$project_dir/Outputs\"\ncreate_folder \"$project_dir/Outputs/Figures\"\ncreate_folder \"$project_dir/Outputs/Tables\"\ncreate_folder \"$project_dir/Outputs/Text\"\n\n* Print confirmation message\ndi \"Project directory and subfolders have been created successfully.\"\n```\n\nSave your raw datasets in ..Data/Raw folder.",
    "crumbs": [
      "Reproducible Research",
      "Using STATA"
    ]
  },
  {
    "objectID": "using-stata.html#variable-creation-and-merging-datasets",
    "href": "using-stata.html#variable-creation-and-merging-datasets",
    "title": "Using STATA",
    "section": "Variable Creation and Merging Datasets",
    "text": "Variable Creation and Merging Datasets\nUse the file here with apporpriate modifications.",
    "crumbs": [
      "Reproducible Research",
      "Using STATA"
    ]
  },
  {
    "objectID": "interactions.html",
    "href": "interactions.html",
    "title": "Interactions",
    "section": "",
    "text": "Understanding\nInteractions occur when the effect of one independent variable (IV) on the dependent variable (DV) varies depending on the level or value of another IV. This means the relationship between one IV and the DV is contingent upon, or moderated by, another variable.\nConceptually, interactions embody the idea of conditional effects, captured succinctly as relationships that depend on context.\nExample: The effect of economic development (GDP per capita) on democratization might depend on the strength of civil society within a country.\nSubstantively, interactions represent moderation1, where one variable (the moderator) alters the strength, direction, or presence of the relationship between another independent variable and the dependent variable. The moderator is essentially a contextual or contingent factor determining when, how strongly, or even whether a relationship holds.\nExample: The impact of trade openness on income inequality could be moderated by the presence or strength of labor unions. Countries with strong labor unions may experience different inequality outcomes compared to those with weak labor unions.",
    "crumbs": [
      "Interactions"
    ]
  },
  {
    "objectID": "interactions.html#interpretation",
    "href": "interactions.html#interpretation",
    "title": "Interactions",
    "section": "Interpretation",
    "text": "Interpretation\nStatistically, interactions are modeled through multiplicative terms in regression equations. Formally, an interaction between variables ( X ) and ( Z ) can be represented as:\n\\[\nY = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3(X \\times Z) + \\epsilon\n\\]\n\nThe marginal effect of ( X ) depends explicitly on ( Z ):\n\\[\n\\frac{\\partial Y}{\\partial X} = \\beta_1 + \\beta_3Z\n\\]\nSimilarly, the marginal effect of ( Z ) depends on ( X ): \\[\n\\frac{\\partial Y}{\\partial Z} = \\beta_2 + \\beta_3X\n\\]\n\nHere, \\(\\beta_3\\) represents the interaction effect, indicating how the relationship between \\(X\\) and \\(Y\\) changes with different levels of \\(Z\\).\n\n\n\n\n\n\nInteractions formally quantify how relationships change across conditions.\n\n\n\nInteractions are used to capture and test conditional hypotheses. Simplest notion of conditional hypothesis is:\n\\(H_1: \\text{An increase in X is associated with an increase in Y when condition Z is met, but not when condition Z is absent}\\)",
    "crumbs": [
      "Interactions"
    ]
  },
  {
    "objectID": "interactions.html#illustrative-example",
    "href": "interactions.html#illustrative-example",
    "title": "Interactions",
    "section": "Illustrative Example",
    "text": "Illustrative Example\nStatistically, if examining how government transparency ((X)) affects foreign direct investment ((Y)), and hypothesizing that this effect depends on political stability ((Z)), the interaction term ((X Z)) formally tests and quantifies this conditional relationship.\n\\[\n\\text{FDI} = \\beta_{0} + \\beta_{1}\\,\\text{Transparency} + \\beta_{2}\\,\\text{Stability} + \\beta_{3}\\,(\\text{Transparency} \\times \\text{Stability}) + \\epsilon\n\\] â€”\n\n\nCode\n# Load libraries\nlibrary(tidyverse)\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate dummy data\nn &lt;- 100\n\ndata &lt;- tibble(\n  Transparency = runif(n, 0, 10),\n  Stability = rep(c(0, 1), each = n/2),\n  epsilon = rnorm(n, 0, 1),\n  FDI = 2 + 0.3*Transparency + 1.5*Stability + 0.5*Transparency*Stability + epsilon\n)\n\n# Define label positions\nlabel_data &lt;- tibble(\n  x = c(5.6, 8, 0, 0),\n  y = c(7.5, 4.2, 2, 3.5),\n  label = c(\"Slope = beta[1] + beta[3]\", \"Slope = beta[1]\", \"beta[0]\", \"beta[2]\"),\n  Stability = factor(c(1, 0, 0, 1))\n)\n\n# Plot the interaction using ggplot\n(p1 &lt;-ggplot(data, aes(x = Transparency, y = FDI, color = factor(Stability))) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x) +\n  geom_text(data = label_data, aes(x = x, y = y, label = label), \n            parse = TRUE, color = \"black\", size = 5, hjust = 0) +\n  scale_color_manual(values = c(\"blue\", \"red\"), labels = c(\"Stability = Low\", \"Stability = High\")) +\n  labs(\n    title = \"Effect of Transparency on FDI Conditional on Stability\",\n    x = \"Government Transparency\",\n    y = \"Foreign Direct Investment (FDI)\",\n    color = \"Political Stability\"\n  ) +\n  annotate(\"text\", x = 3, y = 10, label = \"FDI == (beta[0]+beta[2])+(beta[1]+beta[3])*Transparency~when~Stability==1\", parse = TRUE, hjust = 0, size = 3.5) +\n  annotate(\"text\", x = 3, y = 2.5, label = \"FDI == beta[0]+beta[1]*Transparency~when~Stability==0\", parse = TRUE, hjust = 0, size = 3.5) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  ))\n\n\n\n\n\n\n\n\n\nThe data and relationship displayed in the plot above was generated using the following equation \\[\n\\text{FDI} = 2 + 0.3\\,(\\text{Transparency}) + 1.5\\,(\\text{Stability}) + 0.5\\,(\\text{Transparency} \\times \\text{Stability}) + \\epsilon\n\\]\nPartial derivatives showing interaction effects:\n\\[\n\\frac{\\partial \\text{FDI}}{\\partial \\text{Transparency}} = 0.3 + 0.5\\,(\\text{Stability})\n\\]\nSpecifically, when Stability is Low (0):\n\\[\n\\frac{\\partial \\text{FDI}}{\\partial \\text{Transparency}} = 0.3\n\\]\nAnd when Stability is High (1):\n\\[\n\\frac{\\partial \\text{FDI}}{\\partial \\text{Transparency}} = 0.3 + 0.5 = 0.8\n\\] â€”\nLetâ€™s see what do these coefficients correspond to,\n\n\nCode\n# Define label positions\nlabel_data_2 &lt;- tibble(\n  x = c(5.6, 8, 0, 0),\n  y = c(7.5, 4.2, 2, 3.5),\n  label = c(\"Slope = 0.3 + 0.5\", \"Slope = 0.3\", \"2\", \"+1.5\"),\n  Stability = factor(c(1, 0, 0, 1))\n)\n# Plot the interaction using ggplot\np2 &lt;- ggplot(data, aes(x = Transparency, y = FDI, color = factor(Stability))) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x) +\n  geom_text(data = label_data_2, aes(x = x, y = y, label = label), \n            parse = TRUE, color = \"black\", size = 5, hjust = 0) +\n  scale_color_manual(values = c(\"blue\", \"red\"), labels = c(\"Stability = Low\", \"Stability = High\")) +\n  labs(\n    title = \"Effect of Transparency on FDI Conditional on Stability\",\n    x = \"Government Transparency\",\n    y = \"Foreign Direct Investment (FDI)\",\n    color = \"Political Stability\"\n  ) +\n  annotate(\"text\", x = 3, y = 10, label = \"FDI == (2+1.5)+(0.3+0.5)*Transparency~when~Stability==1\", parse = TRUE, hjust = 0, size = 3.5) +\n  annotate(\"text\", x = 3, y = 2.5, label = \"FDI == 2+0.3*Transparency~when~Stability==0\", parse = TRUE, hjust = 0, size = 3.5) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\nlibrary(gridExtra)\n\ngrid.arrange(p1, p2, nrow=2)",
    "crumbs": [
      "Interactions"
    ]
  },
  {
    "objectID": "interactions.html#simulated-data",
    "href": "interactions.html#simulated-data",
    "title": "Interactions",
    "section": "Simulated Data",
    "text": "Simulated Data\nWe first generate simulated data for both binary and continuous moderators.\n\nR CodeStata Code\n\n\n\nlibrary(tidyverse)\nset.seed(123)\nn &lt;- 200\n\n# Generate simulated dataset\ndata &lt;- tibble(\n  Transparency = runif(n, 0, 10),                             # Continuous independent variable\n  Stability_Binary = rbinom(n, 1, 0.5),                       # Binary moderator\n  Stability_Cont = runif(n, 0, 10),                           # Continuous moderator\n  epsilon = rnorm(n)                                          # Random error term\n) %&gt;% mutate(\n  # Outcome variable with binary moderator interaction\n  FDI_binaryMod = 2 + 0.4*Transparency + 1.2*Stability_Binary + 0.6*(Transparency*Stability_Binary) + epsilon,\n  # Outcome variable with continuous moderator interaction\n  FDI_contMod = 1 + 0.5*Transparency + 0.3*Stability_Cont + 0.4*(Transparency*Stability_Cont) + epsilon\n)\n\n\n\nclear\nset obs 200\nset seed 123\n\n// Generate simulated dataset\ngen Transparency = runiform()*10                    // Continuous independent variable\ngen Stability_Binary = (runiform() &gt; 0.5)           // Binary moderator\ngen Stability_Cont = runiform()*10                  // Continuous moderator\ngen epsilon = rnormal()                             // Random error term\n\n// Outcome variable with binary moderator interaction\ngen FDI_binaryMod = 2 + 0.4*Transparency + 1.2*Stability_Binary + 0.6*(Transparency*Stability_Binary) + epsilon\n\n// Outcome variable with continuous moderator interaction\ngen FDI_contMod = 1 + 0.5*Transparency + 0.3*Stability_Cont + 0.4*(Transparency*Stability_Cont) + epsilon",
    "crumbs": [
      "Interactions"
    ]
  },
  {
    "objectID": "interactions.html#continuous-binary-interaction",
    "href": "interactions.html#continuous-binary-interaction",
    "title": "Interactions",
    "section": "Continuous-Binary Interaction",
    "text": "Continuous-Binary Interaction\n\nLinear Model\nWe estimate an OLS model to analyze how the binary moderator affects the relationship between Transparency and FDI.\n\nR CodeStata Code\n\n\n\n# Linear regression model with binary moderator\nmodel_binary &lt;- lm(FDI_binaryMod ~ Transparency * Stability_Binary, data = data)\nsummary(model_binary)\n\n\nCall:\nlm(formula = FDI_binaryMod ~ Transparency * Stability_Binary, \n    data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82901 -0.66817  0.04696  0.68887  2.44328 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    2.21076    0.20156  10.968  &lt; 2e-16 ***\nTransparency                   0.35538    0.03427  10.369  &lt; 2e-16 ***\nStability_Binary               1.21044    0.30522   3.966 0.000103 ***\nTransparency:Stability_Binary  0.61978    0.05337  11.613  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.015 on 196 degrees of freedom\nMultiple R-squared:  0.8869,    Adjusted R-squared:  0.8852 \nF-statistic: 512.5 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n// Linear regression model with binary moderator\nreg FDI_binaryMod c.Transparency##i.Stability_Binary\n\n\n\nInterpretation: The interaction term shows how the relationship between Transparency and FDI differs across Stability_Binary groups (0 vs.Â 1).\nStatistically, the interaction term (Transparency:Stability_Cont) has a coefficient of 0.4047 and is highly statistically significant (p &lt; 0.001). This indicates strong statistical evidence that the effect of Transparency on FDI increases significantly as Stability_Cont increases.\nSubstantively, the positive interaction term suggests that higher political stability (Stability_Cont) enhances the beneficial impact of government transparency (Transparency) on attracting foreign direct investment (FDI). In other words, transparency leads to even greater increases in FDI when combined with a stable political environment.\n\n\nPredicted Outcome Plot (Binary Moderator)\nWe visualize how predicted FDI varies across Transparency levels for each group defined by the binary moderator.\n\nR CodeStata Code\n\n\n\n#install.packages(\"interactions\")\nlibrary(interactions)\n\n# Using interactions package\ninteract_plot(model_binary, pred = Transparency, modx = Stability_Binary, plot.points = TRUE)\n\n\n\n\n\n\n\n# ggplot equivalent\nggplot(data, aes(x = Transparency, y = FDI_binaryMod, color = factor(Stability_Binary))) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Predicted FDI (Binary Moderator)\",\n       y = \"Predicted FDI\", color = \"Stability (Binary)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n// Predicted outcome plot\nmargins Stability_Binary, at(Transparency=(0(1)10))\nmarginsplot, xdimension(Transparency)",
    "crumbs": [
      "Interactions"
    ]
  },
  {
    "objectID": "interactions.html#continuous-continuous-interaction",
    "href": "interactions.html#continuous-continuous-interaction",
    "title": "Interactions",
    "section": "Continuous-Continuous Interaction",
    "text": "Continuous-Continuous Interaction\n\nLinear Model\nNext, we analyze an interaction where both Transparency and Stability are continuous variables.\n\nR CodeStata Code\n\n\n\n# Linear regression model with continuous moderator\nmodel_cont &lt;- lm(FDI_contMod ~ Transparency * Stability_Cont, data = data)\nsummary(model_cont)\n\n\nCall:\nlm(formula = FDI_contMod ~ Transparency * Stability_Cont, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.87317 -0.62032  0.01226  0.70717  2.52245 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 1.345824   0.301091   4.470 1.32e-05 ***\nTransparency                0.438657   0.053059   8.267 2.04e-14 ***\nStability_Cont              0.276734   0.048990   5.649 5.62e-08 ***\nTransparency:Stability_Cont 0.404681   0.008925  45.342  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.016 on 196 degrees of freedom\nMultiple R-squared:  0.9892,    Adjusted R-squared:  0.9891 \nF-statistic:  5998 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n// Linear regression model with continuous moderator\nreg FDI_contMod c.Transparency##c.Stability_Cont\n\n\n\n\n\nPredicted Outcome Plot (Continuous Moderator)\nWe visualize how predicted FDI changes across Transparency at different values of the continuous moderator Stability.\n\nR CodeStata Code\n\n\n\n# Using interactions package\ninteract_plot(model_cont, pred = Transparency, modx = Stability_Cont, plot.points = TRUE)\n\n\n\n\n\n\n\n# ggplot equivalent with grouped moderator\ndata %&gt;%\n  mutate(Stability_Group = cut(Stability_Cont, breaks = quantile(Stability_Cont, probs = c(0,0.33,0.66,1)))) %&gt;%\n  ggplot(aes(x = Transparency, y = FDI_contMod, color = Stability_Group)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Predicted FDI (Continuous Moderator)\",\n       y = \"Predicted FDI\", color = \"Stability (Grouped)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n// Predicted outcome plot\nmargins, at(Transparency=(0(2)10) Stability_Cont=(0 5 10))\nmarginsplot, xdimension(Transparency)",
    "crumbs": [
      "Interactions"
    ]
  },
  {
    "objectID": "interactions.html#plotting-marginal-effects",
    "href": "interactions.html#plotting-marginal-effects",
    "title": "Interactions",
    "section": "Plotting Marginal Effects",
    "text": "Plotting Marginal Effects\nRecall, the marginal effect of ( X ) depends explicitly on ( Z ):\n\\[\n    \\frac{\\partial Y}{\\partial X} = \\beta_1 + \\beta_3Z\n\\] The coefficient of our main variable of interest \\(X\\) changes at different values of Z. Letâ€™s vizualize that.\n\nBinary Moderator Example\n\nR CodeStata Code\n\n\nThe interplot package allows visualization of how the marginal effect of one variable on the outcome changes across the values of another (moderator) variable.\nFirst, ensure youâ€™ve installed the package:\n\n#install.packages(\"interplot\")\nlibrary(interplot)\n\n# Plot marginal effect of Transparency on FDI for each Stability_Binary category\ninterplot(m = model_binary, var1 = \"Transparency\", var2 = \"Stability_Binary\") +\n  labs(x = \"Stability (Binary)\", \n       y = \"Marginal Effect of Transparency\", \n       title = \"Marginal Effect of Transparency by Stability (Binary)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n// Marginal effect plot\nmargins Stability_Binary, dydx(Transparency)\nmarginsplot, ///\n    xlabel(0 1, valuelabel) ///\n    title(\"Marginal Effect of Transparency by Stability (Binary)\") ///\n    xtitle(\"Stability (Binary)\") ///\n    ytitle(\"Marginal Effect of Transparency\")\n\n\n\nInterpretation: This plot displays the marginal effect of Transparency on FDI for the two groups defined by the binary Stability moderator. Points represent the marginal effects, with lines indicating the 95% confidence intervals.\n\n\nContinuous Moderator Example\n\nR CodeStata Code\n\n\n\nlibrary(interplot)\n\n# Plot marginal effect of Transparency on FDI across Stability_Cont\ninterplot(m = model_cont, var1 = \"Transparency\", var2 = \"Stability_Cont\") +\n  labs(x = \"Stability (Continuous)\", \n       y = \"Marginal Effect of Transparency\", \n       title = \"Marginal Effect of Transparency across Stability (Continuous)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n// Marginal effect plot\nmargins, dydx(Transparency) at(Stability_Cont=(0(1)10))\nmarginsplot, ///\n    xlabel(0(1)10) ///\n    title(\"Marginal Effect of Transparency across Stability (Continuous)\") ///\n    xtitle(\"Stability (Continuous)\") ///\n    ytitle(\"Marginal Effect of Transparency\")\n\n\n\nInterpretation: This plot shows how the marginal effect (coefficient) of Transparency on FDI changes at different values of Stability (Continuous). The shaded area (in R) or error bars (in Stata) indicates the 95% confidence interval.",
    "crumbs": [
      "Interactions"
    ]
  },
  {
    "objectID": "interactions.html#footnotes",
    "href": "interactions.html#footnotes",
    "title": "Interactions",
    "section": "",
    "text": "Moderation is a widely-used interpretation.However, interactions are a versatile analytical tool capable of capturing diverse relationships between variables, including complementarity, substitution, context-dependency, nonlinearity, and effect heterogeneityâ†©ï¸\nAbstract:Multiplicative interaction models are common in the quantitative political science literature. This is so for good reason. Institutional arguments frequently imply that the relationship between political inputs and outcomes varies depending on the institutional context. Models of strategic interaction typically produce conditional hypotheses as well. Although conditional hypotheses are ubiquitous in political science and multiplicative interaction models have been found to capture their intuition quite well, a survey of the top three political science journals from 1998 to 2002 suggests that the execution of these models is often flawed and inferential errors are common. We believe that considerable progress in our understanding of the political world can occur if scholars follow the simple checklist of dos and donâ€™ts for using multiplicative interaction models presented in this article. Only 10% of the articles in our survey followed the checklist.â†©ï¸\nAbstract:When a researcher suspects that the marginal effect of x on y varies with z, a common approach is to plot âˆ‚y/âˆ‚x at different values of z along with a pointwise confidence interval generated using the procedure described in Brambor, Clark, and Golder to assess the magnitude and statistical significance of the relationship. Our article makes three contributions. First, we demonstrate that the Brambor, Clark, and Golder approach produces statistically significant findings when âˆ‚y/âˆ‚x=0 at a rate that can be many times larger or smaller than the nominal false positive rate of the test. Second, we introduce the interactionTest software package for R to implement procedures that allow easy control of the false positive rate. Finally, we illustrate our findings by replicating an empirical analysis of the relationship between ethnic heterogeneity and the number of political parties from Comparative Political Studies.â†©ï¸\nAbstract:Multiplicative interaction models are widely used in social science to examine whether the relationship between an outcome and an independent variable changes with a moderating variable. Current empirical practice tends to overlook two important problems. First, these models assume a linear interaction effect that changes at a constant rate with the moderator. Second, estimates of the conditional effects of the independent variable can be misleading if there is a lack of common support of the moderator. Replicating 46 interaction effects from 22 recent publications in five top political science journals, we find that these core assumptions often fail in practice, suggesting that a large portion of findings across all political science subfields based on interaction models are fragile and model dependent. We propose a checklist of simple diagnostics to assess the validity of these assumptions and offer flexible estimation strategies that allow for nonlinear interaction effects and safeguard against excessive extrapolation. These statistical routines are available in both R and STATA.â†©ï¸",
    "crumbs": [
      "Interactions"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brambor, Thomas, William Roberts Clark, and Matt Golder. 2006.\nâ€œUnderstanding Interaction Models: Improving Empirical\nAnalyses.â€ Political Analysis 14 (1): 63â€“82. https://www.jstor.org/stable/25791835.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig.\n2013. â€œTen Simple Rules for Reproducible Computational\nResearch.â€ PLOS Computational Biology 9 (10): e1003285.\nhttps://doi.org/10.1371/journal.pcbi.1003285.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "rep-research.html",
    "href": "rep-research.html",
    "title": "Reproducible Research",
    "section": "",
    "text": "Why Programming or Coding?\nThere are a lot of fancy answers to it. But the key idea is that you want to be lazy about repetitive tasks (MBAs call it being â€œproductiveâ€).\nMany tasks - data cleaning, wrangling, visualization, and statistical analysis - require you to do them many times. Moreover, you would want to be able to reproduce and replicate your thinking about all of the tasks mentioned above on many different datasets and sometimes even on the same dataset after some time.\nCoding is about formalizing your thinking about how you treat the data and automating the formalization task to be done repetitively. It improves efficiency, enhances reproducibility, and boosts creativity when it comes to finding new patterns in your data.\nBenchmarks for reproducible data and statistical analyses:1",
    "crumbs": [
      "Reproducible Research"
    ]
  },
  {
    "objectID": "rep-research.html#why-programming-or-coding",
    "href": "rep-research.html#why-programming-or-coding",
    "title": "Reproducible Research",
    "section": "",
    "text": "Accuracy: Write a code that reduces the chances of making an error and lets you catch one if it occurs.\nEfficiency: If you are doing it twice, see the pattern of your decision-making and formalize it in your code. Difference between Excel and coding\nReplicate-and-Reproduce: Ability to repeat the computational process which reflects your thinking and decisions that you took along the way. Improves transparency and forces one to be deliberate and responsible about choices during analyses.\n\nHuman Interpretability: Writing code is not just about analyzing but allowing yourself and then others to be able to understand your analytic choices.\n\nPublic Good: Research is a public good. And the code allows your research to be truly accessible. This means you write a code that anyone else who understands the language can read, reuse, and recreate without you being present. We essentially ensure that by writing a readable and ideally publicly accessible code.",
    "crumbs": [
      "Reproducible Research"
    ]
  },
  {
    "objectID": "rep-research.html#guidelines",
    "href": "rep-research.html#guidelines",
    "title": "Reproducible Research",
    "section": "Guidelines",
    "text": "Guidelines\nThe article â€œTen Simple Rules for Reproducible Computational Researchâ€ by Sandve et al. (2013) provides guidelines to ensure that computational research is reproducible, transparent, and robust. Hereâ€™s a summary of the key points:\n\n\n\n\n\n\n\n\nRule\nDescription\nNotes\n\n\n\n\nDocumentation\nTrack how results are produced, including all steps in the analysis workflow.\nKeep short notes on reults\n\n\nAutomation\nMinimize manual data manipulation by using scripts and documenting any manual changes.\nMake changes to raw data in your scripts\n\n\nVersion Control\nUse version control systems for all custom scripts to track changes and maintain reproducibility.\nUsing Github\n\n\nComprehensive Records\nArchive all versions of external programs used, all intermediate results, and exact observation conditions.\nKeep notes about data in comments\n\n\nAccessibility\nMake raw data, scripts, and results publicly accessible to enhance transparency and replication.\nMaintainig good workflow",
    "crumbs": [
      "Reproducible Research"
    ]
  },
  {
    "objectID": "rep-research.html#annotatating-code",
    "href": "rep-research.html#annotatating-code",
    "title": "Reproducible Research",
    "section": "Annotatating Code",
    "text": "Annotatating Code\nA comment should explain the purpose of a command or code and not just be a description of what it does.\nIn R, comments are designated by a # (pound) sign\n\nR - Bad CommentingR - Good Commenting\n\n\n```{r}\nx &lt;- rnorm(100)  # generating data\ny &lt;- x + rnorm(100, mean=50, sd=0.1)  # creating y\nplot(x, y)  # plotting x against y\nm &lt;- lm(y ~ x)  # linear model\nsummary(m)  # summary of model\n```\n\n\n```{r}\n# Generate a sample of 100 random numbers from a standard normal distribution\nx &lt;- rnorm(100)\n\n# Create a dependent variable 'y' with a strong linear relationship plus small random noise\ny &lt;- x + rnorm(100, mean=50, sd=0.1)\n\n# Plot 'x' against 'y' to visualize the relationship\nplot(x, y, main=\"Scatterplot of Y against X\", xlab=\"X variable\", ylab=\"Y variable\")\n\n# Fit a linear regression model to predict 'y' based on 'x'\nmodel &lt;- lm(y ~ x)\n\n# Display a detailed summary of the regression model\nsummary(model)\n\n```\n\n\n\n\nIn STATA Comments may be added to programs in three ways:\n* begin the line with ;\n begin the comment with //;\n* place the comment between /* and */ delimiters.\n\nStata - Bad CommentingStata - Good Commenting\n\n\n```{stata}\nsysuse auto\ngen z = price + weight\nregress z mpg\n\n```\n\n\n```{stata}\n// Load the built-in 'auto' dataset from Stata's system files\nsysuse auto, clear\n\n// Generate a new variable 'z' representing the sum of 'price' and 'weight'\ngen z = price + weight\n\n// Perform a linear regression of 'z' on 'mpg' to understand the relationship between mileage and the new variable\nregress z mpg\n\n```",
    "crumbs": [
      "Reproducible Research"
    ]
  },
  {
    "objectID": "rep-research.html#file-management-and-workflow",
    "href": "rep-research.html#file-management-and-workflow",
    "title": "Reproducible Research",
    "section": "File Management and Workflow",
    "text": "File Management and Workflow\n\nUnderstanding Absolute and Relative Paths\nWhen working with files in any programming environment, paths specify the location of files and folders. These paths can be absolute or relative, and the choice between them significantly impacts reproducibility, portability, and ease of collaboration\nAbsolute Paths An absolute path provides the complete address of a file or folder, starting from the root directory of the file system. It tells the software exactly where to find a file, regardless of where the script is run.\nExample: C:/Users/YourName/Documents/Project/Data/raw_data.dta\nRelative Paths A relative path specifies the location of a file or folder relative to a â€œbase directoryâ€ (e.g., the projectâ€™s working directory). It does not start from the root directory but instead is calculated based on the location of the script.\nSuppose your working directory is set to: C:/Users/YourName/Documents/Project\nThen, a relative path might look like: Data/raw_data.dta\n\n\n\n\n\n\nPractical Analogy Think of absolute and relative paths like giving directions to a house:\nAbsolute Path: â€œGo to the main city square, then take the highway north, turn right at the first traffic light, and find the house at 123 Main Street.â€ Works for people starting anywhere, but requires detailed instructions specific to the city. Relative Path: â€œFrom the library, walk two blocks north, then turn left. The house is the second one on the right.â€ Simpler and context-aware, but assumes everyone starts from the library.\n\n\n\nKey Differences Between Absolute and Relative Paths\n\n\n\n\n\n\n\n\nFeature\nAbsolute Path\nRelative Path\n\n\n\n\nStarting Point\nStarts from the root directory of the file system.\nStarts from the current working directory.\n\n\nPortability\nNot portableâ€”specific to the userâ€™s system.\nHighly portableâ€”adapts to different systems.\n\n\nEase of Sharing\nHarder to share; others must update paths.\nEasier to share; no changes needed if structure is consistent.\n\n\nUse Case\nBest for fixed environments or one-off scripts.\nIdeal for collaborative and reproducible projects.\n\n\nFlexibility\nBreaks if the file is moved or the system changes.\nAdapts as long as the folder structure remains consistent.\n\n\n\n\n\nStandardized Folder and File Structure\nAn efficient file and folder management system is going to be crucial as we move into working with serious projects. Storing using all the files associated with a project in a comprehensible folder system is facilitated in both R and Stata. You would ideally want to create your own template for folder management that you follow across proejcts. For starters, the folder structure below is the one created for your research project in this course.\nYou can use the point-and-click functionality in your computers to create this structure. Or you can do it prgrammatically given the scripts in sub-chapters for stata and R.\nğŸ“¦ cpe-4980-dataessay\nâ”œâ”€Â cpe-4980-dataessay.RProj\nâ”œâ”€Â 000-setup.R\nâ”œâ”€Â 001-eda.qmd\nâ”œâ”€Â 002-analysis.qmd\nâ””â”€Â 003-manuscript.qmd\nâ”œâ”€Â Data\nâ”‚Â Â â”œâ”€Â Raw\nâ”‚Â Â â”‚Â Â â”œâ”€Â Dataset1\nâ”‚Â Â â”‚Â Â â”‚Â Â â”œâ”€Â dataset1.csv\nâ”‚Â Â â”‚Â Â â”‚Â Â â””â”€Â codebook-dataset1.pdf\nâ”‚Â Â â”‚Â Â â””â”€Â Dataset2\nâ”‚Â Â â”‚Â Â Â Â Â â”œâ”€Â ...dta\nâ”‚Â Â â”‚Â Â Â Â Â â””â”€Â codebook-dataset2.pdf\nâ”‚Â Â â””â”€Â Clean\nâ”‚Â Â Â Â Â â””â”€Â Merged-df1-df2.csv\nâ”œâ”€Â Scripts\nâ”‚Â Â â”œâ”€Â R-scripts\nâ”‚Â Â â”‚Â Â â”œâ”€Â plotting-some-variable.R\nâ”‚Â Â â”‚Â Â â””â”€Â exploring-different-models.R\nâ”‚Â Â â”œâ”€Â Stata-Scripts\nâ”‚Â Â â”‚Â Â â””â”€Â seeing-variable-labels.do\nâ”‚Â Â â””â”€Â Python-Scripts\nâ”‚Â Â Â Â Â â””â”€Â scraping-data-from-website.py\nâ””â”€Â Outputs\nÂ Â Â â”œâ”€Â Plots\nÂ Â Â â”‚Â Â â”œâ”€Â ...jpeg\nÂ Â Â â”‚Â Â â””â”€Â ...png\nÂ Â Â â”œâ”€Â Tables\nÂ Â Â â”‚Â Â â””â”€Â .csv\nÂ Â Â â””â”€Â Text\nÂ Â Â Â Â Â â””â”€Â ...txt\n\n\n\n\n\n\nKey Takeaways for Reproduciblility\n\n\n\n\nAny modifications to the raw dataset, like manipulating a variableâ€™s scale, generation of new variables, or removal of values, should be done in the scripts as far as possible. When done externally, make a note with comments in the code.\nWrite well commented code. Explaining the functions performed by the commands in the context of your data analysis.\nUse relative paths.\nUse a standardized folder system.\n\n\n\n\n\n\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig. 2013. â€œTen Simple Rules for Reproducible Computational Research.â€ PLOS Computational Biology 9 (10): e1003285. https://doi.org/10.1371/journal.pcbi.1003285.",
    "crumbs": [
      "Reproducible Research"
    ]
  },
  {
    "objectID": "rep-research.html#footnotes",
    "href": "rep-research.html#footnotes",
    "title": "Reproducible Research",
    "section": "",
    "text": "Inspired by the summary provided by Prof Aaron Williamsâ€™ course on Data Analysis offered at McCourt School. Strongly recommended to learn good coding using Râ†©ï¸",
    "crumbs": [
      "Reproducible Research"
    ]
  }
]